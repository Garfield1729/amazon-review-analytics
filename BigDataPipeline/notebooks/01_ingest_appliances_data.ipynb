{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c45e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2c12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AmazonAppliancesIngestionNotebook\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c7fded7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- image: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- style: struct (nullable = true)\n",
      " |    |-- Color:: string (nullable = true)\n",
      " |    |-- Design:: string (nullable = true)\n",
      " |    |-- Flavor:: string (nullable = true)\n",
      " |    |-- Format:: string (nullable = true)\n",
      " |    |-- Item Package Quantity:: string (nullable = true)\n",
      " |    |-- Length:: string (nullable = true)\n",
      " |    |-- Package Quantity:: string (nullable = true)\n",
      " |    |-- Package Type:: string (nullable = true)\n",
      " |    |-- Pattern:: string (nullable = true)\n",
      " |    |-- Scent:: string (nullable = true)\n",
      " |    |-- Size Name:: string (nullable = true)\n",
      " |    |-- Size:: string (nullable = true)\n",
      " |    |-- Style Name:: string (nullable = true)\n",
      " |    |-- Style:: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      " |-- verified: boolean (nullable = true)\n",
      " |-- vote: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df = spark.read.json(\"file:///home/talentum/projects/amazon-review-analytics/big-data-pipeline/data/raw/Appliances.json\")\n",
    "reviews_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d023cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+----------+-------+--------------------+--------------------+--------------+\n",
      "|    reviewerID|verified|      asin|overall|          reviewText|             summary|unixReviewTime|\n",
      "+--------------+--------+----------+-------+--------------------+--------------------+--------------+\n",
      "|A3NHUQ33CFH3VM|   false|1118461304|    5.0|Not one thing in ...|Clear on what lea...|    1385510400|\n",
      "|A3SK6VNBQDNBJE|   false|1118461304|    5.0|I have enjoyed Dr...|Becoming more inn...|    1383264000|\n",
      "|A3SOFHUR27FO3K|   false|1118461304|    5.0|Alan Gregerman be...|The World from Di...|    1381363200|\n",
      "|A1HOG1PYCAE157|   false|1118461304|    5.0|Alan Gregerman is...|Strangers are You...|    1381276800|\n",
      "|A26JGAM6GZMM4V|   false|1118461304|    5.0|As I began to rea...|How and why it is...|    1378512000|\n",
      "+--------------+--------+----------+-------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+--------------------+-------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|summary|          reviewerID|               asin|           overall|          reviewText|             summary|      unixReviewTime|\n",
      "+-------+--------------------+-------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|  count|              602777|             602777|            602777|              602453|              602649|              602777|\n",
      "|   mean|                null|     2.6397998868E9|4.2742092017445925|  1.3421052631578947|2.4517103615384616E7|1.4550493403165681E9|\n",
      "| stddev|                null|3.007506265129408E9|1.3015969744055793|  1.5815885922409423|   8.7604279199858E7| 5.481787076532877E7|\n",
      "|    min|A0001528BGUBOEVR6T5U|         1118461304|               1.0|\n",
      "I recently purch...|                    |           972259200|\n",
      "|    max|       AZZZY1W55XHZR|         B01HJHHQM6|               5.0|~We purchased thi...|~~~ MADE in USA ~...|          1538611200|\n",
      "+-------+--------------------+-------------------+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df_clean = reviews_df.select(\n",
    "    \"reviewerID\",\n",
    "    \"verified\",\n",
    "    \"asin\",\n",
    "    \"overall\",\n",
    "    \"reviewText\",\n",
    "    \"summary\",\n",
    "    \"unixReviewTime\"\n",
    ")\n",
    "\n",
    "reviews_df_clean.show(5)\n",
    "reviews_df_clean.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "861fb491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----+-------+----------+-------+--------------+\n",
      "|reviewerID|verified|asin|overall|reviewText|summary|unixReviewTime|\n",
      "+----------+--------+----+-------+----------+-------+--------------+\n",
      "|         0|       0|   0|      0|         0|      0|             0|\n",
      "+----------+--------+----+-------+----------+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "602777"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count, size\n",
    "\n",
    "def get_empty_condition(column_name, data_type):\n",
    "    # If it's a standard String column\n",
    "    if data_type == 'string':\n",
    "        return col(column_name) == \"\"\n",
    "    # If it's an Array (like your 'category' column)\n",
    "    elif 'array' in data_type:\n",
    "        return size(col(column_name)) == 0\n",
    "    # For other types (int, float, etc.), they can't be \"empty\", only null\n",
    "    else:\n",
    "        return col(column_name).isNull()\n",
    "\n",
    "# Apply the logic dynamically to meta_df_safe\n",
    "reviews_df_clean.select([\n",
    "    count(when(get_empty_condition(c, t), c)).alias(c) \n",
    "    for c, t in reviews_df_clean.dtypes\n",
    "]).show()\n",
    "\n",
    "reviews_df_clean.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdda84a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "563870"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = reviews_df_clean.filter(col(\"verified\") == \"true\")\n",
    "df_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61356fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews_df_clean.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"file:///home/talentum/projects/amazon-review-analytics/big-data-pipeline/data/cleaned/reviews_bronze\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01f2eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- also_buy: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- also_view: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- category: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- description: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- details: struct (nullable = true)\n",
      " |    |-- \n",
      "    Item Weight: \n",
      "    : string (nullable = true)\n",
      " |    |-- \n",
      "    Product Dimensions: \n",
      "    : string (nullable = true)\n",
      " |    |-- ASIN:: string (nullable = true)\n",
      " |    |-- ASIN: : string (nullable = true)\n",
      " |    |-- Batteries: string (nullable = true)\n",
      " |    |-- Domestic Shipping: : string (nullable = true)\n",
      " |    |-- International Shipping: : string (nullable = true)\n",
      " |    |-- Item model number:: string (nullable = true)\n",
      " |    |-- Publisher:: string (nullable = true)\n",
      " |    |-- Shipping Advisory:: string (nullable = true)\n",
      " |    |-- Shipping Weight:: string (nullable = true)\n",
      " |    |-- UPC:: string (nullable = true)\n",
      " |-- feature: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- fit: string (nullable = true)\n",
      " |-- imageURL: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- imageURLHighRes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- main_cat: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- rank: string (nullable = true)\n",
      " |-- similar_item: string (nullable = true)\n",
      " |-- tech1: string (nullable = true)\n",
      " |-- tech2: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Metadata Handling\n",
    "\n",
    "#Read Metadata\n",
    "meta_df = spark.read.json(\"file:///home/talentum/projects/amazon-review-analytics/big-data-pipeline/data/raw/meta_Appliances.json\")\n",
    "\n",
    "meta_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b37c13c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+-------+--------------------+-----------+\n",
      "|      asin|               title|         brand|  price|            category|   main_cat|\n",
      "+----------+--------------------+--------------+-------+--------------------+-----------+\n",
      "|7301113188|Tupperware Freeze...|    Tupperware|       |[Appliances, Refr...| Appliances|\n",
      "|7861850250|2 X Tupperware Pu...|    Tupperware|  $3.62|[Appliances, Refr...| Appliances|\n",
      "|8792559360|The Cigar - Momen...|The Cigar Book|$150.26|[Appliances, Part...|Amazon Home|\n",
      "+----------+--------------------+--------------+-------+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select SAFE Columns Only\n",
    "\n",
    "meta_df_safe = meta_df.select(\n",
    "    \"asin\",\n",
    "    \"title\",\n",
    "    \"brand\",\n",
    "    \"price\",\n",
    "    \"category\",\n",
    "    \"main_cat\"\n",
    ")\n",
    "meta_df_safe.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cb8ba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+-----+--------+--------+\n",
      "|asin|title|brand|price|category|main_cat|\n",
      "+----+-----+-----+-----+--------+--------+\n",
      "|   0|    0|  584|10292|     806|      91|\n",
      "+----+-----+-----+-----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count, size\n",
    "\n",
    "def get_empty_condition(column_name, data_type):\n",
    "    # If it's a standard String column\n",
    "    if data_type == 'string':\n",
    "        return col(column_name) == \"\"\n",
    "    # If it's an Array (like your 'category' column)\n",
    "    elif 'array' in data_type:\n",
    "        return size(col(column_name)) == 0\n",
    "    # For other types (int, float, etc.), they can't be \"empty\", only null\n",
    "    else:\n",
    "        return col(column_name).isNull()\n",
    "\n",
    "# Apply the logic dynamically to meta_df_safe\n",
    "meta_df_safe.select([\n",
    "    count(when(get_empty_condition(c, t), c)).alias(c) \n",
    "    for c, t in meta_df_safe.dtypes\n",
    "]).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d398a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_filtered=meta_df_safe.drop(\"price\",\"main_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2194538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+--------+\n",
      "|asin|title|brand|category|\n",
      "+----+-----+-----+--------+\n",
      "|   0|    0|  584|     806|\n",
      "+----+-----+-----+--------+\n",
      "\n",
      "+----------+--------------------+----------------+--------------------+\n",
      "|      asin|               title|           brand|            category|\n",
      "+----------+--------------------+----------------+--------------------+\n",
      "|B00068FG8C|Bottle Caps Bemis...|           Bemis|[Appliances, Part...|\n",
      "|B0006HKZ2U|Bemis 5560890 Pow...|           Bemis|[Appliances, Part...|\n",
      "|B0006HKX72|Bemis Genuine Rep...|                |[Appliances, Part...|\n",
      "|B0006HKX6I|Bemis Humidifier ...|                |[Appliances, Part...|\n",
      "|B0006V8ML2|Bemis 1041 Humidi...|                |[Appliances, Part...|\n",
      "|B0009GVY8W|Bemis Humidifier ...|           Bemis|[Appliances, Part...|\n",
      "|B0009GZK4G|BEM1040/BEM1042 B...|           Bemis|[Appliances, Part...|\n",
      "|B0009H5ZYA|Bemis Humidifier ...|           Bemis|[Appliances, Part...|\n",
      "|B000BPK5FO|Bemis Water Wheel...|      Essick Air|[Appliances, Part...|\n",
      "|B000DZ9TTK|Bemis 9000 Series...|         BestAir|[Appliances, Part...|\n",
      "|B000LF4ALY|Bemis Humidifier ...|           Bemis|[Appliances, Part...|\n",
      "|B004X7CM4I|Wick Basket Repla...|           Bemis|[Appliances, Part...|\n",
      "|B007KDXX5O|Bemis 1041 Humidi...|           Bemis|[Appliances, Part...|\n",
      "|B009CENRQO|Genuine OEM Bemis...|Bemis/Essick Air|[Appliances, Part...|\n",
      "|B00W29K12E|arVin BEM1045 Bem...|           arVin|[Appliances, Part...|\n",
      "|B016DUWKKI|Humidifier Filter...|             EFP|[Appliances, Part...|\n",
      "|B016DUWKLC|Humidifier Filter...|             EFP|[Appliances, Part...|\n",
      "|B0185N7JBG|Bemis Wick Filter...|    RPS PRODUCTS|[Appliances, Part...|\n",
      "+----------+--------------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_empty_condition(column_name, data_type):\n",
    "    # If it's a standard String column\n",
    "    if data_type == 'string':\n",
    "        return col(column_name) == \"\"\n",
    "    # If it's an Array (like your 'category' column)\n",
    "    elif 'array' in data_type:\n",
    "        return size(col(column_name)) == 0\n",
    "    # For other types (int, float, etc.), they can't be \"empty\", only null\n",
    "    else:\n",
    "        return col(column_name).isNull()\n",
    "\n",
    "# Apply the logic dynamically to meta_df_safe\n",
    "meta_df_filtered.select([\n",
    "    count(when(get_empty_condition(c, t), c)).alias(c) \n",
    "    for c, t in meta_df_filtered.dtypes\n",
    "]).show()\n",
    "meta_df_filtered.filter(col(\"title\").contains(\"Bemis\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0cfffe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve 'trim(`category`)' due to data type mismatch: argument 1 requires string type, however, '`category`' is of array<string> type.;;\\n'Project [asin#545, title#561, brand#546, category#547, brand_clean#725, CASE WHEN NOT ((isnull(category#547) || (trim(category#547, None) = )) || lower(trim(category#547, None)) IN (null,none)) THEN category#547 END AS category_clean#731]\\n+- Project [asin#545, title#561, brand#546, category#547, CASE WHEN NOT ((isnull(brand#546) || (trim(brand#546, None) = )) || lower(trim(brand#546, None)) IN (null,none)) THEN brand#546 END AS brand_clean#725]\\n   +- Project [asin#545, title#561, brand#546, category#547]\\n      +- Project [asin#545, title#561, brand#546, price#556, category#547, main_cat#555]\\n         +- Relation[also_buy#543,also_view#544,asin#545,brand#546,category#547,date#548,description#549,details#550,feature#551,fit#552,imageURL#553,imageURLHighRes#554,main_cat#555,price#556,rank#557,similar_item#558,tech1#559,tech2#560,title#561] json\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o210.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve 'trim(`category`)' due to data type mismatch: argument 1 requires string type, however, '`category`' is of array<string> type.;;\n'Project [asin#545, title#561, brand#546, category#547, brand_clean#725, CASE WHEN NOT ((isnull(category#547) || (trim(category#547, None) = )) || lower(trim(category#547, None)) IN (null,none)) THEN category#547 END AS category_clean#731]\n+- Project [asin#545, title#561, brand#546, category#547, CASE WHEN NOT ((isnull(brand#546) || (trim(brand#546, None) = )) || lower(trim(brand#546, None)) IN (null,none)) THEN brand#546 END AS brand_clean#725]\n   +- Project [asin#545, title#561, brand#546, category#547]\n      +- Project [asin#545, title#561, brand#546, price#556, category#547, main_cat#555]\n         +- Relation[also_buy#543,also_view#544,asin#545,brand#546,category#547,date#548,description#549,details#550,feature#551,fit#552,imageURL#553,imageURLHighRes#554,main_cat#555,price#556,rank#557,similar_item#558,tech1#559,tech2#560,title#561] json\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$2(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$13.apply(TreeNode.scala:356)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:356)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2258)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-09c589ce3a25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     .withColumn(\n\u001b[1;32m     18\u001b[0m         \u001b[0;34m\"category_clean\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mis_invalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"category\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"category\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     ) \\\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1995\u001b[0m         \"\"\"\n\u001b[1;32m   1996\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1997\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve 'trim(`category`)' due to data type mismatch: argument 1 requires string type, however, '`category`' is of array<string> type.;;\\n'Project [asin#545, title#561, brand#546, category#547, brand_clean#725, CASE WHEN NOT ((isnull(category#547) || (trim(category#547, None) = )) || lower(trim(category#547, None)) IN (null,none)) THEN category#547 END AS category_clean#731]\\n+- Project [asin#545, title#561, brand#546, category#547, CASE WHEN NOT ((isnull(brand#546) || (trim(brand#546, None) = )) || lower(trim(brand#546, None)) IN (null,none)) THEN brand#546 END AS brand_clean#725]\\n   +- Project [asin#545, title#561, brand#546, category#547]\\n      +- Project [asin#545, title#561, brand#546, price#556, category#547, main_cat#555]\\n         +- Relation[also_buy#543,also_view#544,asin#545,brand#546,category#547,date#548,description#549,details#550,feature#551,fit#552,imageURL#553,imageURLHighRes#554,main_cat#555,price#556,rank#557,similar_item#558,tech1#559,tech2#560,title#561] json\\n\""
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim, lower\n",
    "\n",
    "def is_invalid(col_name):\n",
    "    return (\n",
    "        col(col_name).isNull() |\n",
    "        (trim(col(col_name)) == \"\") |\n",
    "        (lower(trim(col(col_name))).isin(\"null\", \"none\"))\n",
    "    )\n",
    "\n",
    "from pyspark.sql.functions import when, first\n",
    "\n",
    "reference_df = meta_df_filtered \\\n",
    "    .withColumn(\n",
    "        \"brand_clean\",\n",
    "        when(~is_invalid(\"brand\"), col(\"brand\"))\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"category_clean\",\n",
    "        when(~is_invalid(\"category\"), col(\"category\"))\n",
    "    ) \\\n",
    "    .groupBy(\"title\") \\\n",
    "    .agg(\n",
    "        first(\"brand_clean\", ignorenulls=True).alias(\"brand_inferred\"),\n",
    "        first(\"category_clean\", ignorenulls=True).alias(\"category_inferred\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d07b557",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meta_df_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c8ad1b9dfe72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m meta_df_enriched = meta_df_final.join(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mreference_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'meta_df_final' is not defined"
     ]
    }
   ],
   "source": [
    "meta_df_enriched = meta_df_final.join(\n",
    "    reference_df,\n",
    "    on=\"title\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "meta_df_enriched = meta_df_enriched \\\n",
    "    .withColumn(\n",
    "        \"brand\",\n",
    "        when(is_invalid(\"brand\"), col(\"brand_inferred\"))\n",
    "        .otherwise(col(\"brand\"))\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"category\",\n",
    "        when(is_invalid(\"category\"), col(\"category_inferred\"))\n",
    "        .otherwise(col(\"category\"))\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "meta_df_enriched = meta_df_enriched.fillna({\n",
    "    \"brand\": \"unknown\",\n",
    "    \"category\": \"unknown\"\n",
    "})\n",
    "\n",
    "\n",
    "profile_empty_values(meta_df_enriched).show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_condition(column_name, data_type):\n",
    "    # If it's a standard String column\n",
    "    if data_type == 'string':\n",
    "        return col(column_name) == \"\"\n",
    "    # If it's an Array (like your 'category' column)\n",
    "    elif 'array' in data_type:\n",
    "        return size(col(column_name)) == 0\n",
    "    # For other types (int, float, etc.), they can't be \"empty\", only null\n",
    "    else:\n",
    "        return col(column_name).isNull()\n",
    "\n",
    "# Apply the logic dynamically to meta_df_safe\n",
    "meta_df_brand_filled.select([\n",
    "    count(when(get_empty_condition(c, t), c)).alias(c) \n",
    "    for c, t in meta_df_brand_filled.dtypes\n",
    "]).show()\n",
    "\n",
    "meta_df_brand_filled.filter(col(\"title\").contains(\"Bemis\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33283e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "meta_df_norm2 = meta_df_brand_filled.withColumn(\n",
    "    \"category\",\n",
    "    when(\n",
    "        col(\"category\").isNull() | (size(col(\"category\")) == 0),\n",
    "        None\n",
    "    ).otherwise(col(\"category\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db153662",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_by_title = meta_df_norm2 \\\n",
    "    .filter(col(\"category\").isNotNull()) \\\n",
    "    .groupBy(\"title\") \\\n",
    "    .agg(first(\"category\", ignorenulls=True).alias(\"category_from_title\"))\n",
    "\n",
    "\n",
    "\n",
    "meta_df_final_clean = meta_df_norm2 \\\n",
    "    .join(category_by_title, on=\"title\", how=\"left\") \\\n",
    "    .withColumn(\n",
    "        \"category\",\n",
    "        when(col(\"category\").isNull(), col(\"category_from_title\"))\n",
    "        .otherwise(col(\"category\"))\n",
    "    ) \\\n",
    "    .drop(\"category_from_title\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434e4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_condition(column_name, data_type):\n",
    "    # If it's a standard String column\n",
    "    if data_type == 'string':\n",
    "        return col(column_name) == \"\"\n",
    "    # If it's an Array (like your 'category' column)\n",
    "    elif 'array' in data_type:\n",
    "        return size(col(column_name)) == 0\n",
    "    # For other types (int, float, etc.), they can't be \"empty\", only null\n",
    "    else:\n",
    "        return col(column_name).isNull()\n",
    "\n",
    "# Apply the logic dynamically to meta_df_safe\n",
    "meta_df_final_clean.select([\n",
    "    count(when(get_empty_condition(c, t), c)).alias(c) \n",
    "    for c, t in meta_df_final_clean.dtypes\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da814cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Metadata Bronze Parquet\n",
    "\n",
    "meta_df_safe.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"file:///home/talentum/projects/amazon-review-analytics/big-data-pipeline/data/cleaned/meta_bronze\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e5fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
