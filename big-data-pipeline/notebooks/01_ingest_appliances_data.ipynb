{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262a4db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89eff4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AmazonAppliancesIngestionNotebook\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fdf31ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- image: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- style: struct (nullable = true)\n",
      " |    |-- Color:: string (nullable = true)\n",
      " |    |-- Design:: string (nullable = true)\n",
      " |    |-- Flavor:: string (nullable = true)\n",
      " |    |-- Format:: string (nullable = true)\n",
      " |    |-- Item Package Quantity:: string (nullable = true)\n",
      " |    |-- Length:: string (nullable = true)\n",
      " |    |-- Package Quantity:: string (nullable = true)\n",
      " |    |-- Package Type:: string (nullable = true)\n",
      " |    |-- Pattern:: string (nullable = true)\n",
      " |    |-- Scent:: string (nullable = true)\n",
      " |    |-- Size Name:: string (nullable = true)\n",
      " |    |-- Size:: string (nullable = true)\n",
      " |    |-- Style Name:: string (nullable = true)\n",
      " |    |-- Style:: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      " |-- verified: boolean (nullable = true)\n",
      " |-- vote: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df = spark.read.json(\"file:///home/talentum/projects/amazon-review-analytics/big-data-pipeline/data/raw/Appliances.json\")\n",
    "reviews_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46f4b6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+----------+-------+--------------------+--------------------+--------------+\n",
      "|    reviewerID|verified|      asin|overall|          reviewText|             summary|unixReviewTime|\n",
      "+--------------+--------+----------+-------+--------------------+--------------------+--------------+\n",
      "|A3NHUQ33CFH3VM|   false|1118461304|    5.0|Not one thing in ...|Clear on what lea...|    1385510400|\n",
      "|A3SK6VNBQDNBJE|   false|1118461304|    5.0|I have enjoyed Dr...|Becoming more inn...|    1383264000|\n",
      "|A3SOFHUR27FO3K|   false|1118461304|    5.0|Alan Gregerman be...|The World from Di...|    1381363200|\n",
      "|A1HOG1PYCAE157|   false|1118461304|    5.0|Alan Gregerman is...|Strangers are You...|    1381276800|\n",
      "|A26JGAM6GZMM4V|   false|1118461304|    5.0|As I began to rea...|How and why it is...|    1378512000|\n",
      "+--------------+--------+----------+-------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+--------------------+-------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|summary|          reviewerID|               asin|           overall|          reviewText|             summary|      unixReviewTime|\n",
      "+-------+--------------------+-------------------+------------------+--------------------+--------------------+--------------------+\n",
      "|  count|              602777|             602777|            602777|              602453|              602649|              602777|\n",
      "|   mean|                null|     2.6397998868E9|4.2742092017445925|  1.3421052631578947|2.4517103615384616E7|1.4550493403165681E9|\n",
      "| stddev|                null|3.007506265129408E9|1.3015969744055793|  1.5815885922409423|   8.7604279199858E7| 5.481787076532877E7|\n",
      "|    min|A0001528BGUBOEVR6T5U|         1118461304|               1.0|\n",
      "I recently purch...|                    |           972259200|\n",
      "|    max|       AZZZY1W55XHZR|         B01HJHHQM6|               5.0|~We purchased thi...|~~~ MADE in USA ~...|          1538611200|\n",
      "+-------+--------------------+-------------------+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df_clean = reviews_df.select(\n",
    "    \"reviewerID\",\n",
    "    \"verified\",\n",
    "    \"asin\",\n",
    "    \"overall\",\n",
    "    \"reviewText\",\n",
    "    \"summary\",\n",
    "    \"unixReviewTime\"\n",
    ")\n",
    "\n",
    "reviews_df_clean.show(5)\n",
    "reviews_df_clean.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d2cc901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----+-------+----------+-------+--------------+\n",
      "|reviewerID|verified|asin|overall|reviewText|summary|unixReviewTime|\n",
      "+----------+--------+----+-------+----------+-------+--------------+\n",
      "|         0|       0|   0|      0|         0|      0|             0|\n",
      "+----------+--------+----+-------+----------+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "602777"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count, size\n",
    "\n",
    "def get_empty_condition(column_name, data_type):\n",
    "    # If it's a standard String column\n",
    "    if data_type == 'string':\n",
    "        return col(column_name) == \"\"\n",
    "    # If it's an Array (like your 'category' column)\n",
    "    elif 'array' in data_type:\n",
    "        return size(col(column_name)) == 0\n",
    "    # For other types (int, float, etc.), they can't be \"empty\", only null\n",
    "    else:\n",
    "        return col(column_name).isNull()\n",
    "\n",
    "# Apply the logic dynamically to meta_df_safe\n",
    "reviews_df_clean.select([\n",
    "    count(when(get_empty_condition(c, t), c)).alias(c) \n",
    "    for c, t in reviews_df_clean.dtypes\n",
    "]).show()\n",
    "\n",
    "reviews_df_clean.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ef2bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "563870"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = reviews_df_clean.filter(col(\"verified\") == \"true\")\n",
    "df_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9780dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews_df_clean.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"file:///home/talentum/projects/amazon-review-analytics/big-data-pipeline/data/cleaned/reviews_bronze\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c326f286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- also_buy: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- also_view: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- category: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- description: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- details: struct (nullable = true)\n",
      " |    |-- \n",
      "    Item Weight: \n",
      "    : string (nullable = true)\n",
      " |    |-- \n",
      "    Product Dimensions: \n",
      "    : string (nullable = true)\n",
      " |    |-- ASIN:: string (nullable = true)\n",
      " |    |-- ASIN: : string (nullable = true)\n",
      " |    |-- Batteries: string (nullable = true)\n",
      " |    |-- Domestic Shipping: : string (nullable = true)\n",
      " |    |-- International Shipping: : string (nullable = true)\n",
      " |    |-- Item model number:: string (nullable = true)\n",
      " |    |-- Publisher:: string (nullable = true)\n",
      " |    |-- Shipping Advisory:: string (nullable = true)\n",
      " |    |-- Shipping Weight:: string (nullable = true)\n",
      " |    |-- UPC:: string (nullable = true)\n",
      " |-- feature: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- fit: string (nullable = true)\n",
      " |-- imageURL: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- imageURLHighRes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- main_cat: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- rank: string (nullable = true)\n",
      " |-- similar_item: string (nullable = true)\n",
      " |-- tech1: string (nullable = true)\n",
      " |-- tech2: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Metadata Handling\n",
    "\n",
    "#Read Metadata\n",
    "meta_df = spark.read.json(\"file:///home/talentum/projects/amazon-review-analytics/big-data-pipeline/data/raw/meta_Appliances.json\")\n",
    "\n",
    "meta_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd9e08fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+-------+--------------------+-----------+\n",
      "|      asin|               title|         brand|  price|            category|   main_cat|\n",
      "+----------+--------------------+--------------+-------+--------------------+-----------+\n",
      "|7301113188|Tupperware Freeze...|    Tupperware|       |[Appliances, Refr...| Appliances|\n",
      "|7861850250|2 X Tupperware Pu...|    Tupperware|  $3.62|[Appliances, Refr...| Appliances|\n",
      "|8792559360|The Cigar - Momen...|The Cigar Book|$150.26|[Appliances, Part...|Amazon Home|\n",
      "+----------+--------------------+--------------+-------+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select SAFE Columns Only\n",
    "\n",
    "meta_df_safe = meta_df.select(\n",
    "    \"asin\",\n",
    "    \"title\",\n",
    "    \"brand\",\n",
    "    \"price\",\n",
    "    \"category\",\n",
    "    \"main_cat\"\n",
    ")\n",
    "meta_df_safe.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aca07608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+-----+--------+--------+\n",
      "|asin|title|brand|price|category|main_cat|\n",
      "+----+-----+-----+-----+--------+--------+\n",
      "|   0|    0|  584|10292|     806|      91|\n",
      "+----+-----+-----+-----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count, size\n",
    "\n",
    "def get_empty_condition(column_name, data_type):\n",
    "    # If it's a standard String column\n",
    "    if data_type == 'string':\n",
    "        return col(column_name) == \"\"\n",
    "    # If it's an Array (like your 'category' column)\n",
    "    elif 'array' in data_type:\n",
    "        return size(col(column_name)) == 0\n",
    "    # For other types (int, float, etc.), they can't be \"empty\", only null\n",
    "    else:\n",
    "        return col(column_name).isNull()\n",
    "\n",
    "# Apply the logic dynamically to meta_df_safe\n",
    "meta_df_safe.select([\n",
    "    count(when(get_empty_condition(c, t), c)).alias(c) \n",
    "    for c, t in meta_df_safe.dtypes\n",
    "]).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aff6f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_filtered=meta_df_safe.drop(\"price\",\"main_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4174eef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+--------+\n",
      "|asin|title|brand|category|\n",
      "+----+-----+-----+--------+\n",
      "|   0|    0|  584|     806|\n",
      "+----+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_empty_condition(column_name, data_type):\n",
    "    # If it's a standard String column\n",
    "    if data_type == 'string':\n",
    "        return col(column_name) == \"\"\n",
    "    # If it's an Array (like your 'category' column)\n",
    "    elif 'array' in data_type:\n",
    "        return size(col(column_name)) == 0\n",
    "    # For other types (int, float, etc.), they can't be \"empty\", only null\n",
    "    else:\n",
    "        return col(column_name).isNull()\n",
    "\n",
    "# Apply the logic dynamically to meta_df_safe\n",
    "meta_df_filtered.select([\n",
    "    count(when(get_empty_condition(c, t), c)).alias(c) \n",
    "    for c, t in meta_df_filtered.dtypes\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a91e64a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, when\n",
    "\n",
    "meta_df_norm = meta_df_filtered.withColumn(\n",
    "    \"brand\",\n",
    "    when(\n",
    "        col(\"brand\").isNull() | (trim(col(\"brand\")) == \"\"),\n",
    "        None\n",
    "    ).otherwise(col(\"brand\"))\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "brand_by_title = meta_df_norm \\\n",
    "    .filter(col(\"brand\").isNotNull()) \\\n",
    "    .groupBy(\"title\") \\\n",
    "    .agg(first(\"brand\", ignorenulls=True).alias(\"brand_from_title\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a03b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_brand_filled = meta_df_norm \\\n",
    "    .join(brand_by_title, on=\"title\", how=\"left\") \\\n",
    "    .withColumn(\n",
    "        \"brand\",\n",
    "        when(col(\"brand\").isNull(), col(\"brand_from_title\"))\n",
    "        .otherwise(col(\"brand\"))\n",
    "    ) \\\n",
    "    .drop(\"brand_from_title\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f47e7b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+--------+\n",
      "|title|asin|brand|category|\n",
      "+-----+----+-----+--------+\n",
      "|    0|   0|    0|     806|\n",
      "+-----+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_empty_condition(column_name, data_type):\n",
    "    # If it's a standard String column\n",
    "    if data_type == 'string':\n",
    "        return col(column_name) == \"\"\n",
    "    # If it's an Array (like your 'category' column)\n",
    "    elif 'array' in data_type:\n",
    "        return size(col(column_name)) == 0\n",
    "    # For other types (int, float, etc.), they can't be \"empty\", only null\n",
    "    else:\n",
    "        return col(column_name).isNull()\n",
    "\n",
    "# Apply the logic dynamically to meta_df_safe\n",
    "meta_df_brand_filled.select([\n",
    "    count(when(get_empty_condition(c, t), c)).alias(c) \n",
    "    for c, t in meta_df_brand_filled.dtypes\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "552ea389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "meta_df_norm2 = meta_df_brand_filled.withColumn(\n",
    "    \"category\",\n",
    "    when(\n",
    "        col(\"category\").isNull() | (size(col(\"category\")) == 0),\n",
    "        None\n",
    "    ).otherwise(col(\"category\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49c6e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_by_title = meta_df_norm2 \\\n",
    "    .filter(col(\"category\").isNotNull()) \\\n",
    "    .groupBy(\"title\") \\\n",
    "    .agg(first(\"category\", ignorenulls=True).alias(\"category_from_title\"))\n",
    "\n",
    "\n",
    "\n",
    "meta_df_final_clean = meta_df_norm2 \\\n",
    "    .join(category_by_title, on=\"title\", how=\"left\") \\\n",
    "    .withColumn(\n",
    "        \"category\",\n",
    "        when(col(\"category\").isNull(), col(\"category_from_title\"))\n",
    "        .otherwise(col(\"category\"))\n",
    "    ) \\\n",
    "    .drop(\"category_from_title\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6848b168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+--------+\n",
      "|title|asin|brand|category|\n",
      "+-----+----+-----+--------+\n",
      "|    0|   0|    0|       0|\n",
      "+-----+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_empty_condition(column_name, data_type):\n",
    "    # If it's a standard String column\n",
    "    if data_type == 'string':\n",
    "        return col(column_name) == \"\"\n",
    "    # If it's an Array (like your 'category' column)\n",
    "    elif 'array' in data_type:\n",
    "        return size(col(column_name)) == 0\n",
    "    # For other types (int, float, etc.), they can't be \"empty\", only null\n",
    "    else:\n",
    "        return col(column_name).isNull()\n",
    "\n",
    "# Apply the logic dynamically to meta_df_safe\n",
    "meta_df_final_clean.select([\n",
    "    count(when(get_empty_condition(c, t), c)).alias(c) \n",
    "    for c, t in meta_df_final_clean.dtypes\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8617a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Metadata Bronze Parquet\n",
    "\n",
    "meta_df_safe.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"file:///home/talentum/projects/amazon-review-analytics/big-data-pipeline/data/cleaned/meta_bronze\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804cce00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
